{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains information and research regarding the Keras machine learning API.  \n",
    "The notebook will explain each Keras function that is used, referring to the References at the foot of the notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction  \n",
    "  \n",
    "Keras is a deep learning API with a Python interface to work with artificial neural networks.  \n",
    "Keras acts as an interface for the TensorFlow library, developed by the Google Brain team for the purposes of machine learning and artificial intelligence.  \n",
    "Keras was created by Francois Chollet, a software engineer at Google, and the coding is regularly maintained on GitHub.  \n",
    "Keras was created to make it easier for people to learn about their workflow and explain errors concisely.  \n",
    "\n",
    "Neural Networks  \n",
    "  \n",
    "Keras was created in order to better understand the deep learning of neural networks, which are a series of algorithms that intend to recognise trends in data which mimic how the human brain would perceive such trends from data.  \n",
    "Neural networks utilise multiple operators in nodes to assist in breaking down problems into smaller ones which can be solved individually.  \n",
    "Keras allows users to develop neural networks on the Internet, on iOS and Android systems as well as the Java Virtual Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries  \n",
    "  \n",
    "We will provide Keras with data to read from, via the Numenta Anomaly Benchmark (NAB) dataset.  \n",
    "The NAB intends to provide sequences of artificial timeseries data which contains labelled anomalous periods of behaviour, all in order, all timestamped and all possessing single values.  \n",
    "\n",
    "Timeseries are the sequences of numerical data, these were collected at successive times, every 5 minutes for a period of 14 days.  \n",
    "Timeseries are important for the purposes of predictions, data can be used by businesses such as the stock market in order to better understand results and make corrections to help improve their business.  \n",
    "\n",
    "Anomaly Detection  \n",
    "  \n",
    "Anomaly detection means developing a program that can detect any unusually occurring data points in a data set.  \n",
    "We will aim to build an LSTM Autoencoder to visualise and analyse the Keras data set.  \n",
    "LSTM (Long Short-Term Memory) is an artificial neural network that processes data and decides with training whether to keep or forget information.  \n",
    "\n",
    "Setup  \n",
    "\n",
    "First we will train the data without anomalies, then take the new data point and attempt to reconstruct this via the autoencoder.  \n",
    "Next, if the reconstructed error in the new data set is above a designated threshold, it is labelled as an anomaly.  \n",
    "\n",
    "We will use csv files:  \n",
    "First, training is carried out via the small_noise csv, without anomalies.  \n",
    "Second, testing is carried out via the jumpsup_csv, with anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL for the NAB data set\n",
    "nab_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "# training data with small noise & no anomalies\n",
    "df_small_noise_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = nab_url + df_small_noise_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates = True, index_col = \"timestamp\"\n",
    ")\n",
    "\n",
    "# testing data with jumps up & anomalies present\n",
    "df_daily_jumpsup_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = nab_url + df_daily_jumpsup_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates = True, index_col = \"timestamp\"\n",
    ")\n",
    "\n",
    "# printing the timeseries' training and testing data\n",
    "print(df_small_noise.head())\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting timeseries data without anomalies for training\n",
    "fig, ax = plt.subplots()\n",
    "df_small_noise.plot(legend = False, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting timeseries data with anomalies for testing\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend = False, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data values from the training timeseries will be normalized.  \n",
    "Normalization refers to reorganising our dataset in order to remove any redundant data, resulting in a more efficient means of storing the data.  \n",
    "\n",
    "The data values/timesteps occur every 5 minutes for 14 days:  \n",
    "(24 hours * (12 values per hour)) = 288 timesteps per day:  \n",
    "(288 timesteps * 14 days) = 4,032 data points across 14 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing & saving mean and standard deviation (std) to normalize testing data\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences  \n",
    "  \n",
    "We will create sequences from combining timesteps data from the training data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Timesteps per day (12 values per hour * 24 hours per day)\n",
    "time_steps = (12 * 24)\n",
    "\n",
    "# function for generating training sequences for the model\n",
    "def create_sequences(values, time_steps = time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training Input Shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model  \n",
    "\n",
    "The autoencoder model below will take the input of shape below:  \n",
    "  \n",
    "batch_size, sequence_length, num_features  \n",
    "\n",
    "The model will then return output of the same shape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use x_train as both the input and output in this reconstruction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs = 50,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "https://keras.io/  \n",
    "https://keras.io/examples/timeseries/timeseries_anomaly_detection/  \n",
    "https://valueml.com/anomaly-detection-in-time-series-data-using-keras/  \n",
    "https://towardsdatascience.com/time-series-of-price-anomaly-detection-with-lstm-11a12ba4f6d9  \n",
    "https://en.wikipedia.org/wiki/Keras  \n",
    "https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f82d0fd1be96d5c2cfc8b5b6d623de297a3323bdcee05967133f841892dc18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
