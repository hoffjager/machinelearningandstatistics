{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains information and research regarding the Keras machine learning API.  \n",
    "The notebook will explain each Keras function that is used, referring to the documentation on Keras' website:  \n",
    "\n",
    "Keras Team. Keras: the python deep learning api, 2022a.  \n",
    "URL https://keras.io/  \n",
    "Keras Team. Timeseries anomaly detection using an autoencoder,\n",
    "2022b.  \n",
    "URL https://keras.io/examples/timeseries/timeseries_anomaly_detection/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is an open-source software library with a Python interface for artificial neural networks.  \n",
    "Keras acts as an interface for the TensorFlow library, developed by the Google Brain team for the purposes of machine learning and artificial intelligence.  \n",
    "Keras was created by Francois Chollet, a software engineer at Google, and the coding is regularly maintained on GitHub.  \n",
    "\n",
    "Keras was created in order to better understand neural networks, which are a series of algorithms that intend to recognise trends in data which mimic how the human brain would perceive such trends from data.  \n",
    "Keras allows users to develop neural networks on the Internet, on iOS and Android systems as well as the Java Virtual Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide Keras with data to read from via the Numenta Anomaly Benchmark (NAB) dataset.  \n",
    "\n",
    "The NAB intends to provide sequences of artificial timeseries data which contains labelled anomalous periods of behaviour, all in order, all timestamped and all possessing single values.  \n",
    "Timeseries refers to the sequences of numerical data, these were collected at successive times, in this case every 5 minutes for a period of 14 days.  \n",
    "Timeseries are important for the purposes of predictions, data can be used to better understand results and make further progress in business.  \n",
    "\n",
    "Anomaly detection means developing a program that can detect any unusually occurring data points in a data set.  \n",
    "By using machine learning, we can increase the speed of executing a model network built for anomaly detection.  \n",
    "  \n",
    "We will aim to build an LSTM Autoencoder to visualise and analyse the Keras data set.  \n",
    "LSTM (Long Short-Term Memory) is an artificial neural network that processes data and decides with training whether to keep or forget information.  \n",
    "\n",
    "We will also use csv files:  \n",
    "First, training is carried out via the small noise csv.  \n",
    "Second, testing is carried out via the jumps up csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nab_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "# training via small noise\n",
    "df_small_noise_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = nab_url + df_small_noise_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates = True, index_col = \"timestamp\"\n",
    ")\n",
    "\n",
    "# testing via jumps up\n",
    "df_daily_jumpsup_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = nab_url + df_daily_jumpsup_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates = True, index_col = \"timestamp\"\n",
    ")\n",
    "\n",
    "# printing the training and testing data\n",
    "print(df_small_noise.head())\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting timeseries data without anomalies for training\n",
    "fig, ax = plt.subplots()\n",
    "df_small_noise.plot(legend = False, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting timeseries data with anomalies for testing\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend = False, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get data values from the training csv file and normalize the value data.  \n",
    "Normalization refers to reorganising our dataset in order to remove any redundant data, resulting in a more efficient means of storing the data.  \n",
    "\n",
    "The data values/timesteps occur every 5 minutes for 14 days:  \n",
    "\n",
    "(24 hours * (12 values per hour)) = 288 timesteps per day:  \n",
    "(288 timesteps * 14 days) = 4,032 data points across 14 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing by acquiring the mean and standard deviation (std)\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Timesteps per day (12 values per hour multiplied by 24 hours)\n",
    "time_steps = (12 * 24)\n",
    "\n",
    "# function for generating training sequences for model\n",
    "def create_sequences(values, time_steps = time_steps):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training Input Shape:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder model will take the input of shape and return output of the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = keras.Sequential(\n",
    "[\n",
    "    layers.Input(shape = (x_train.shape[1], x_train.shape[2])),\n",
    "    layers.Conv1D(\n",
    "        filters = 32, kernel_size = 7, padding = \"same\", strides = 2, activation = \"relu\"\n",
    "    ),\n",
    "    layers.Dropout(rate = 0.2),\n",
    "    layers.Conv1D(\n",
    "        filters = 16, kernel_size = 7, padding = \"same\", strides = 2, activation = \"relu\"\n",
    "    ),\n",
    "    layers.Conv1DTranspose(\n",
    "        filters = 16, kernel_size = 7, padding = \"same\", strides = 2, activation = \"relu\"\n",
    "    ),\n",
    "    layers.Dropout(rate = 0.2),\n",
    "    layers.Conv1DTranspose(\n",
    "        filters = 32, kernel_size = 7, padding = \"same\", strides = 2, activation = \"relu\"\n",
    "    ),\n",
    "    layers.Conv1DTranspose(filters = 1, kernel_size = 7, padding = \"same\"),\n",
    "]\n",
    ")\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.001), loss = \"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use x_train as both the input and output in this reconstruction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs = 50,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 5, mode = \"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "https://keras.io/  \n",
    "https://keras.io/examples/timeseries/timeseries_anomaly_detection/  \n",
    "https://valueml.com/anomaly-detection-in-time-series-data-using-keras/  \n",
    "https://towardsdatascience.com/time-series-of-price-anomaly-detection-with-lstm-11a12ba4f6d9  \n",
    "https://en.wikipedia.org/wiki/Keras  \n",
    "https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-keras  \n",
    "https://www.investopedia.com/terms/n/neuralnetwork.asp  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f82d0fd1be96d5c2cfc8b5b6d623de297a3323bdcee05967133f841892dc18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
