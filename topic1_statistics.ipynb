{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lady Tasting Tea Problem  \n",
    "Ronald A. Fisher coined the Lady Tasting Tea Problem, where the lady can ascertain whether tea had milk first or second during the tea preparation phase.  \n",
    "How many combinations of arrangements can the lady make from 8 cups of tea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cups = list(range(8))\n",
    "cups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we or the lady were to select cups randomly, 1 of 70 options are possible.  \n",
    "This is calculated from: (8 x 7 x 6 x 5) / (4 x 3 x 2 x 1) = 70  \n",
    "We will use the itertools( ) package within Python, where all the possibilities/combinations are generated via itertools.combinations (list, number of possibilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cups = list(range(8))\n",
    "poss = list(itertools.combinations(cups, 4))\n",
    "poss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the list (cups) along with a number (4).  \n",
    "This returns every possible way of selecting 4 random cups from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis  \n",
    "Null hypothesis is defined as the test subject being unable to tell if the cup had milk first or second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Hypothesis  \n",
    "If the test subject can choose the 4 cups with milk correctly, there is a 1 in 70 chance of this happening, or approximately 1.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating percentage of alternative hypothesis (1 in 70)\n",
    "(1/70)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1:  \n",
    "The code above gives about a 1.5% chance of randomly selecting the correct cups with milk first.  \n",
    "Calculate the minimum number of cups of tea required to ensure the probability of randomly selecting the correct cups of tea is less than or equal to 1%.  \n",
    "Bonus: How many would be required if you were to allow the taster to get one cup wrong whilst maintaining the 1% threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cups = list(range(100))\n",
    "poss = list(itertools.combinations(cups, 2))\n",
    "\n",
    "print(poss)\n",
    "\n",
    "(1/6) * 100\n",
    "# 1 of 6 from: (4*3)/(2*1) = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing random & plotting functions\n",
    "import random\n",
    "import seaborn\n",
    "\n",
    "milkfirst = set(random.choice(poss))\n",
    "\n",
    "counts = [len(milkfirst & set(i)) for i in itertools.combinations(cups, 4)]\n",
    "\n",
    "seaborn.countplot(x = counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2:  \n",
    "Use scipy's version of Fisher's exact test to simulate the Lady Tasting Tea problem.  \n",
    "Reference:  \n",
    "Zach, How to Perform Fisherâ€™s Exact Test in Python, Statology, https://www.statology.org/fishers-exact-test-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher's Exact Test is used to examine whether or not there is a major association between two variables.  \n",
    "The Scipy Stats library contains the fisher_exact( ) function to perform the test:  \n",
    " \n",
    "Null hypothesis (H0) is where both variables are independent, whereas Alternative hypothesis (H1) is where both variables are not independent.  \n",
    "\n",
    "We calculate the odd_ratio and p_value below.  \n",
    "\n",
    "If the level of significance is 0.05, we do not reject the null hypothesis if the p_value is above 0.05, otherwise the alternative hypothesis is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import scipy.stats as stats\n",
    "\n",
    "cups = list(range(8))\n",
    "poss = list(itertools.combinations(cups, 4))\n",
    "poss\n",
    "\n",
    "odd_ratio, p_value = stats.fisher_exact(poss)\n",
    "\n",
    "print('Odd Ratio: ' + str(odd_ratio))\n",
    "print('p_value: ' + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-tests & Simulated Data  \n",
    "Fake data sets can be created with specific properties to investigate numerical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules for numbering and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters for two different lists of numbers\n",
    "m_a, s_a, m_b, s_b = 1.0, 0.4, 2.0, 0.4\n",
    "\n",
    "# Sample size\n",
    "N = 40\n",
    "\n",
    "# Creating two lists of numbers based on bell-shaped probability curves\n",
    "a = np.random.normal(loc = m_a, scale = s_a, size = N)\n",
    "b = np.random.normal(loc = m_b, scale = s_b, size = N)\n",
    "\n",
    "# Placing both samples into a single dataframe\n",
    "df = pd.DataFrame({'Category': ['A'] * len(a) + ['B'] * len(b), 'Value': np.hstack([a, b])})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Alternative statistics package\n",
    "import statsmodels.stats.weightstats as stat\n",
    "\n",
    "# Main statistics package.\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More sophisticated plotting.\n",
    "import seaborn as sns\n",
    "\n",
    "# Better sized plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Nicer colours and styles for plots.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Visualising data\n",
    "sns.catplot(x = 'Category', y = 'Value', jitter = False, data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running t-test using scipy.stats( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "t_ss, p_ss = ss.ttest_ind(a, b)\n",
    "\n",
    "print(f\"t-value: {t_ss}\\tp-value: {p_ss}\")\n",
    "print(f\"P_scipy: {p_ss: 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running t-test using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.weightstats as stat\n",
    "\n",
    "t_sm, p_sm, d_sm = stat.ttest_ind(a, b)\n",
    "\n",
    "print(f\"t-value: {t_sm}\\tp-value: {p_sm}\\tDeg Free: {d_sm}\")\n",
    "print(f\"P_statsmodels: {p_sm: 0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating t statistic by hand  \n",
    "https://en.wikipedia.org/wiki/Test_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the arrays\n",
    "n1 = len(a)\n",
    "n2 = len(b)\n",
    "\n",
    "# Means of the samples\n",
    "m1 = np.sum(a) / n1\n",
    "m2 = np.sum(b) / n2\n",
    "\n",
    "# Sample standard deviations\n",
    "s1 = np.sqrt(np.sum((a - m1) ** 2) / (n1 - 1))\n",
    "s2 = np.sqrt(np.sum((b - m2) ** 2) / (n2 - 1))\n",
    "\n",
    "df = n1 + n2 - 2\n",
    "sp2 = ((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / df\n",
    "t = (m1 - m2) / (np.sqrt(sp2) * np.sqrt(1.0 / n1 + 1.0 / n2))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populations  \n",
    "Normal Distribution Plots of Two Different Populations  \n",
    "https://en.wikipedia.org/wiki/Normal_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting x values\n",
    "min_x = min(m_a, m_b) - 5.0 * max(s_a, s_b)\n",
    "max_x = max(m_a, m_b) + 5.0 * max(s_a, s_b)\n",
    "x = np.linspace(min_x, max_x, 1000)\n",
    "\n",
    "y_a = ss.norm.pdf(x, m_a, s_a)\n",
    "y_b = ss.norm.pdf(x, m_b, s_b)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.plot(x, y_a)\n",
    "ax.plot(x, y_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critical Value  \n",
    "Critical Value is used to make a decision in relation to the calculated t statistic from samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critical probability value\n",
    "critical = 0.05\n",
    "\n",
    "# Creating the figure\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "# Range of x values, which represent the t statistic\n",
    "min_x = -5.0\n",
    "max_x = 5.0\n",
    "x = np.linspace(min_x, max_x, 1000)\n",
    "\n",
    "# The probability density function (pdf) of the t statistic\n",
    "# using the degrees of freedom listed above and plotting figure\n",
    "t = ss.t.pdf(x, d_sm)\n",
    "\n",
    "ax.plot(x, t, color = 'red')\n",
    "\n",
    "# Getting the tails & plotting them\n",
    "tf = pd.DataFrame({'x': x, 't': t})\n",
    "tcrit = abs(ss.t.ppf(critical / 2.0, d_sm))\n",
    "tail_one = tf[tf['x'] >= tcrit]\n",
    "tail_two = tf[tf['x'] <= -tcrit]\n",
    "\n",
    "ax.fill_between(tail_one['x'], tail_one['t'], 0, facecolor = \"red\")\n",
    "ax.fill_between(tail_two['x'], tail_two['t'], 0, facecolor = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type I Errors: False Positives  \n",
    "Running 10,000 t-tests where the population means are equal.  \n",
    "We should make the wrong decision/reject the hypothesis (100 * critical) % of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 10,000 t-tests to run\n",
    "trials = 10000\n",
    "# Setting 100 values per sample\n",
    "N = 100\n",
    "# Population 1 mean, Population 2 mean, and Standard Deviation for both\n",
    "mean1, mean2, stddev = 2.0, 2.0, 0.3\n",
    "# Critical probability value\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors committed\n",
    "rejects = 0\n",
    "\n",
    "# Looping through the t-tests\n",
    "for i in range(trials):\n",
    "    # Generating Sample 1\n",
    "    sample1 = np.random.normal(loc = mean1, scale = stddev, size = N)\n",
    "    # Generating Sample 2\n",
    "    sample2 = np.random.normal(loc = mean2, scale = stddev, size = N)\n",
    "    # Running t-test\n",
    "    t, p = ss.ttest_ind(sample1, sample2)\n",
    "    # If p is less than or equal to critical, reject it\n",
    "    if p <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Printing results\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type II Errors: False Negatives  \n",
    "Running 10,000 t-tests where the population means are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 10,000 t-tests to run\n",
    "trials = 10000\n",
    "# Setting 100 values per sample\n",
    "N = 100\n",
    "# Population 1 mean, Population 2 mean, and Standard Deviation for both\n",
    "mean1, mean2, stddev = 2.0, 2.1, 0.3\n",
    "# Critical probability value\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type II errors committed\n",
    "notrejects = 0\n",
    "\n",
    "# Looping through the t-tests\n",
    "for i in range(trials):\n",
    "    # Generating Sample 1\n",
    "    sample1 = np.random.normal(loc = mean1, scale = stddev, size = N)\n",
    "    # Generating Sample 2\n",
    "    sample2 = np.random.normal(loc = mean2, scale = stddev, size = N)\n",
    "    # Running t-test\n",
    "    t, p = ss.ttest_ind(sample1, sample2)\n",
    "    # If p is greater than critical, DO NOT reject it\n",
    "    if p > critical:\n",
    "        notrejects = notrejects + 1\n",
    "\n",
    "# Printing results\n",
    "typeii = 100.0 * (notrejects / trials)\n",
    "print(f\"{typeii: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired Samples  \n",
    "Vincent Arel-Bundock's R datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsleep = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sleep.csv\")\n",
    "dfsleep\n",
    "\n",
    "# Extracting the first sample from the data set\n",
    "drugA = dfsleep[dfsleep[\"group\"] == 1]\n",
    "drugA = drugA.sort_values(\"ID\")\n",
    "drugA = drugA[\"extra\"].to_numpy()\n",
    "drugA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsleep = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sleep.csv\")\n",
    "dfsleep\n",
    "\n",
    "# Extracting the second sample from the data set\n",
    "drugB = dfsleep[dfsleep[\"group\"] == 2]\n",
    "drugB = drugB.sort_values(\"ID\")\n",
    "drugB = drugB[\"extra\"].to_numpy()\n",
    "drugB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a paired samples t-test\n",
    "ss.ttest_rel(drugA, drugB)\n",
    "\n",
    "# Equivalent to a one sample t-test\n",
    "ss.ttest_1samp(drugB - drugA, 0)\n",
    "\n",
    "# Suggestion from statsmodels for running the t-test\n",
    "stat.DescrStatsW(drugB - drugA).ttest_mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with multiple t-tests  \n",
    "If we want to compare three groups, and null hypothesis states that all population means are equal.  \n",
    "Can three t-tests run in parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of each sample\n",
    "N = 100\n",
    "\n",
    "# Creating three samples\n",
    "sampA = np.random.normal(1.0, 0.2, N)\n",
    "sampB = np.random.normal(1.0, 0.2, N)\n",
    "sampC = np.random.normal(2.0, 0.2, N)\n",
    "\n",
    "# Putting samples into a single dataframe\n",
    "sample = ['A'] * N + ['B'] * N + ['C'] * N\n",
    "values = np.hstack([sampA, sampB, sampC])\n",
    "dfsamps = pd.DataFrame({'Sample': sample, 'Value': values})\n",
    "\n",
    "# Visualising samples\n",
    "sns.catplot(x = 'Sample', y = 'Value', jitter = False, data = dfsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-tests for each pair of samples\n",
    "t_AB, p_AB = ss.ttest_ind(sampA, sampB)\n",
    "t_AC, p_AC = ss.ttest_ind(sampA, sampC)\n",
    "t_BC, p_BC = ss.ttest_ind(sampB, sampC)\n",
    "\n",
    "print(f\"p_AB: {p_AB: .2f}\\tp_AC: {p_AC: .2f}\\tp_BC: {p_BC: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 10,000 t-tests where the population means are equal  \n",
    "We should make the wrong decision (reject the hypothesis) (100 * critical) % of the time.  \n",
    "We expect to incorrectly reject the null hypothesis 5% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 10,000 t-tests to run\n",
    "trials = 10000\n",
    "# Setting 100 values per sample\n",
    "N = 100\n",
    "# Population 1 mean, Population 2 mean, Population 3 mean, and Standard Deviation for all three means\n",
    "mean1, mean2, mean3, stddev = 2.0, 2.0, 2.0, 0.3\n",
    "# Critical probability value\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors committed\n",
    "rejects = 0\n",
    "\n",
    "# Looping through the t-tests\n",
    "for i in range(trials):\n",
    "    # Generating Sample 1\n",
    "    sample1 = np.random.normal(loc = mean1, scale = stddev, size = N)\n",
    "    # Generating Sample 2\n",
    "    sample2 = np.random.normal(loc = mean2, scale = stddev, size = N)\n",
    "    # Generating Sample 3\n",
    "    sample3 = np.random.normal(loc = mean3, scale = stddev, size = N)\n",
    "    # Running t-tests\n",
    "    t1, p1 = ss.ttest_ind(sample1, sample2)\n",
    "    t2, p2 = ss.ttest_ind(sample1, sample3)\n",
    "    t3, p3 = ss.ttest_ind(sample2, sample3)\n",
    "    # If any are less than or equal to critical, reject them\n",
    "    if p1 <= critical or p2 <= critical or p3 <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Printing results\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA)  \n",
    "ANOVA may be used to avoid a higher Type I error rate: False Positives  \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, P = ss.f_oneway(sampA, sampB, sampC)\n",
    "print(f\"F: {F: .2f} P: {P: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running 10,000 ANOVAs where the population means are equal  \n",
    "We should make the wrong decision (reject the hypothesis) (100 * critical) % of the time.  \n",
    "We expect to incorrectly reject the null hypothesis 5% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 10,000 ANOVAs to run\n",
    "trials = 10000\n",
    "# Setting 100 values per sample\n",
    "N = 100\n",
    "# Population 1 mean, Population 2 mean, Population 3 mean, and Standard Deviation for all three means\n",
    "mean1, mean2, mean3, stddev = 2.0, 2.0, 2.0, 0.3\n",
    "# Critical probability value\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors committed\n",
    "rejects = 0\n",
    "\n",
    "# Looping through the t-tests\n",
    "for i in range(trials):\n",
    "    # Generating Sample 1\n",
    "    sample1 = np.random.normal(loc = mean1, scale = stddev, size = N)\n",
    "    # Generating Sample 2\n",
    "    sample2 = np.random.normal(loc = mean2, scale = stddev, size = N)\n",
    "    # Generating Sample 3\n",
    "    sample3 = np.random.normal(loc = mean3, scale = stddev, size = N)\n",
    "    # Running ANOVA test\n",
    "    F, p = ss.f_oneway(sample1, sample2, sample3)\n",
    "    # If less than or equal to critical, reject it\n",
    "    if p <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Printing results\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei: 0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3:  \n",
    "Take the code from the Examples section of the scipy.stats documentation for Independent Samples t-tests.  \n",
    "Add this to your own notebook and explain how it works by using Markdown cells and code comments.  \n",
    "Improve it in any way you think it could be improved.  \n",
    "Reference:  \n",
    "SciPy Stats t-Test, SciPy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy.stats.ttest_ind(a, b, axis = 0, equal_var = True, nan_policy = 'propagate', permutations = None, random_state = None, alternative = 'two-sided', trim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with sample with identical means\n",
    "rvs1 = stats.norm.rvs(loc = 5, scale = 10, size = 500, random_state = rng)\n",
    "rvs2 = stats.norm.rvs(loc = 5, scale = 10, size = 500, random_state = rng)\n",
    "\n",
    "stats.ttest_ind(rvs1, rvs2)\n",
    "stats.ttest_ind(rvs1, rvs2, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_ind underestimates p for unequal variances\n",
    "rvs3 = stats.norm.rvs(loc = 5, scale = 20, size = 500, random_state = rng)\n",
    "\n",
    "stats.ttest_ind(rvs1, rvs3)\n",
    "stats.ttest_ind(rvs1, rvs3, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When n1 != n2, the equal variance t-statistic is no longer equal to the unequal variance t-statistic\n",
    "rvs4 = stats.norm.rvs(loc = 5, scale = 20, size = 100, random_state = rng)\n",
    "\n",
    "stats.ttest_ind(rvs1, rvs4)\n",
    "stats.ttest_ind(rvs1, rvs4, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test with different means, variance and n\n",
    "rvs5 = stats.norm.rvs(loc = 8, scale = 20, size = 100, random_state = rng)\n",
    "\n",
    "stats.ttest_ind(rvs1, rvs5)\n",
    "stats.ttest_ind(rvs1, rvs5, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When performing a permutation test, more permutations typically yields more accurate results.\n",
    "# Use a np.random.Generator to ensure reproducibility\n",
    "stats.ttest_ind(rvs1, rvs5, permutations = 10000, random_state = rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking two samples, one with an extreme tail\n",
    "a = (56, 128.6, 12, 123.8, 64.34, 78, 763.3)\n",
    "b = (1.1, 2.9, 4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trim keyword to perform a trimmed (Yuen) t-test.\n",
    "# For example, using 20% trimming, trim = .2,\n",
    "# the test will reduce the impact of\n",
    "# one (np.floor(trim*len(a))) element from each tail of sample a.\n",
    "# It will have no effect on sample b because\n",
    "# np.floor(trim*len(b)) is 0.\n",
    "stats.ttest_ind(a, b, trim = .2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1f82d0fd1be96d5c2cfc8b5b6d623de297a3323bdcee05967133f841892dc18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
